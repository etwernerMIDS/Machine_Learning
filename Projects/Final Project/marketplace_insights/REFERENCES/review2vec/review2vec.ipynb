{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: - \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - defaults/linux-64::scikit-learn==0.22.1=py36hd81dba3_0\n",
      "  - defaults/linux-64::bkcharts==0.2=py36_0\n",
      "  - defaults/noarch::dask==2.14.0=py_0\n",
      "  - defaults/noarch::numpydoc==0.9.2=py_0\n",
      "  - defaults/linux-64::pytest-arraydiff==0.3=py36h39e3cac_0\n",
      "  - defaults/linux-64::bottleneck==1.3.2=py36heb32a55_0\n",
      "  - defaults/linux-64::pywavelets==1.1.1=py36h7b6447c_0\n",
      "  - defaults/noarch::pytest-astropy==0.8.0=py_0\n",
      "  - defaults/linux-64::numexpr==2.7.1=py36h423224d_0\n",
      "  - defaults/linux-64::h5py==2.10.0=py36h7918eee_0\n",
      "  - defaults/linux-64::numpy-base==1.18.1=py36hde5b4d6_1\n",
      "  - defaults/noarch::s3fs==0.4.0=py_0\n",
      "  - defaults/linux-64::patsy==0.5.1=py36_0\n",
      "  - defaults/linux-64::scikit-image==0.16.2=py36h0573a6f_0\n",
      "  - defaults/linux-64::matplotlib-base==3.1.3=py36hef1b27d_0\n",
      "  - defaults/linux-64::pytables==3.6.1=py36h71ec239_0\n",
      "  - defaults/linux-64::mkl_fft==1.0.15=py36ha843d7b_0\n",
      "  - defaults/linux-64::statsmodels==0.11.0=py36h7b6447c_0\n",
      "  - defaults/linux-64::bokeh==2.0.1=py36_0\n",
      "  - defaults/noarch::seaborn==0.10.0=py_0\n",
      "  - defaults/linux-64::numba==0.48.0=py36h0573a6f_0\n",
      "  - defaults/linux-64::scipy==1.4.1=py36h0b6359f_0\n",
      "  - defaults/noarch::pytest-doctestplus==0.5.0=py_0\n",
      "  - defaults/noarch::imageio==2.8.0=py_0\n",
      "  - defaults/linux-64::astropy==4.0.1.post1=py36h7b6447c_0\n",
      "  - defaults/linux-64::mkl_random==1.1.0=py36hd6b4f25_0\n",
      "  - defaults/noarch::sphinx==3.0.4=py_0\n",
      "  - defaults/linux-64::spyder==4.1.2=py36_0\n",
      "  - defaults/linux-64::matplotlib==3.1.3=py36_0\n",
      "  - defaults/linux-64::pandas==1.0.3=py36h0573a6f_0\n",
      "  - defaults/linux-64::numpy==1.18.1=py36h4f9e942_0\n",
      "failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: \\ \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - defaults/noarch::numpydoc==0.9.2=py_0\n",
      "  - defaults/noarch::s3fs==0.4.0=py_0\n",
      "  - defaults/noarch::sphinx==3.0.4=py_0\n",
      "  - defaults/linux-64::spyder==4.1.2=py36_0\n",
      "done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/python3\n",
      "\n",
      "  added / updated specs:\n",
      "    - pytorch\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _pytorch_select-0.2        |            gpu_0           2 KB\n",
      "    cudnn-7.6.5                |       cuda10.1_0       179.9 MB\n",
      "    ninja-1.10.0               |   py36hfd86e86_0         1.4 MB\n",
      "    pytorch-1.4.0              |cuda101py36h02f0884_0       167.4 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       348.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _pytorch_select    pkgs/main/linux-64::_pytorch_select-0.2-gpu_0\n",
      "  boto3              pkgs/main/noarch::boto3-1.14.12-py_0\n",
      "  botocore           pkgs/main/noarch::botocore-1.17.12-py_0\n",
      "  cudatoolkit        pkgs/main/linux-64::cudatoolkit-10.1.243-h6bb024c_0\n",
      "  cudnn              pkgs/main/linux-64::cudnn-7.6.5-cuda10.1_0\n",
      "  docutils           pkgs/main/linux-64::docutils-0.15.2-py36_0\n",
      "  ninja              pkgs/main/linux-64::ninja-1.10.0-py36hfd86e86_0\n",
      "  pytorch            pkgs/main/linux-64::pytorch-1.4.0-cuda101py36h02f0884_0\n",
      "  s3transfer         pkgs/main/linux-64::s3transfer-0.3.3-py36_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates                                2020.1.1-0 --> 2020.6.24-0\n",
      "  certifi                                 2020.4.5.2-py36_0 --> 2020.6.20-py36_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "_pytorch_select-0.2  | 2 KB      | ##################################### | 100% \n",
      "ninja-1.10.0         | 1.4 MB    | ##################################### | 100% \n",
      "cudnn-7.6.5          | 179.9 MB  | ##################################### | 100% \n",
      "pytorch-1.4.0        | 167.4 MB  | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "! conda install -y pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: | \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - defaults/linux-64::scikit-learn==0.22.1=py36hd81dba3_0\n",
      "  - defaults/linux-64::bkcharts==0.2=py36_0\n",
      "  - defaults/noarch::dask==2.14.0=py_0\n",
      "  - defaults/linux-64::pytorch==1.4.0=cuda101py36h02f0884_0\n",
      "  - defaults/linux-64::pytest-arraydiff==0.3=py36h39e3cac_0\n",
      "  - defaults/linux-64::bottleneck==1.3.2=py36heb32a55_0\n",
      "  - defaults/linux-64::pywavelets==1.1.1=py36h7b6447c_0\n",
      "  - defaults/noarch::pytest-astropy==0.8.0=py_0\n",
      "  - defaults/linux-64::numexpr==2.7.1=py36h423224d_0\n",
      "  - defaults/linux-64::h5py==2.10.0=py36h7918eee_0\n",
      "  - defaults/linux-64::numpy-base==1.18.1=py36hde5b4d6_1\n",
      "  - defaults/linux-64::patsy==0.5.1=py36_0\n",
      "  - defaults/linux-64::scikit-image==0.16.2=py36h0573a6f_0\n",
      "  - defaults/linux-64::matplotlib-base==3.1.3=py36hef1b27d_0\n",
      "  - defaults/linux-64::pytables==3.6.1=py36h71ec239_0\n",
      "  - defaults/linux-64::mkl_fft==1.0.15=py36ha843d7b_0\n",
      "  - defaults/linux-64::statsmodels==0.11.0=py36h7b6447c_0\n",
      "  - defaults/linux-64::bokeh==2.0.1=py36_0\n",
      "  - defaults/noarch::seaborn==0.10.0=py_0\n",
      "  - defaults/linux-64::numba==0.48.0=py36h0573a6f_0\n",
      "  - defaults/linux-64::scipy==1.4.1=py36h0b6359f_0\n",
      "  - defaults/noarch::pytest-doctestplus==0.5.0=py_0\n",
      "  - defaults/noarch::imageio==2.8.0=py_0\n",
      "  - defaults/linux-64::astropy==4.0.1.post1=py36h7b6447c_0\n",
      "  - defaults/linux-64::mkl_random==1.1.0=py36hd6b4f25_0\n",
      "  - defaults/linux-64::matplotlib==3.1.3=py36_0\n",
      "  - defaults/linux-64::pandas==1.0.3=py36h0573a6f_0\n",
      "  - defaults/linux-64::numpy==1.18.1=py36h4f9e942_0\n",
      "failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/python3\n",
      "\n",
      "  added / updated specs:\n",
      "    - faiss-cpu\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    boto3-1.9.66               |           py36_0         105 KB\n",
      "    docutils-0.16              |           py36_0         668 KB\n",
      "    faiss-cpu-1.6.3            |   py36h6bb024c_0         2.1 MB  pytorch\n",
      "    s3transfer-0.1.13          |           py36_0          79 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.0 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  faiss-cpu          pytorch/linux-64::faiss-cpu-1.6.3-py36h6bb024c_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  docutils                                    0.15.2-py36_0 --> 0.16-py36_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  boto3                pkgs/main/noarch::boto3-1.14.12-py_0 --> pkgs/main/linux-64::boto3-1.9.66-py36_0\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "  botocore                                     1.17.12-py_0 --> 1.12.189-py_0\n",
      "  s3transfer                                   0.3.3-py36_0 --> 0.1.13-py36_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "s3transfer-0.1.13    | 79 KB     | ##################################### | 100% \n",
      "faiss-cpu-1.6.3      | 2.1 MB    | ##################################### | 100% \n",
      "docutils-0.16        | 668 KB    | ##################################### | 100% \n",
      "boto3-1.9.66         | 105 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "! conda install faiss-cpu -y -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/ec2-user/anaconda3/envs/JupyterSystemEnv:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "_libgcc_mutex             0.1                        main  \n",
      "asn1crypto                1.3.0            py36h9f0ad1d_1    conda-forge\n",
      "attrs                     19.3.0                     py_0    conda-forge\n",
      "autovizwidget             0.12.9                     py_0    conda-forge\n",
      "awscli                    1.18.104                 pypi_0    pypi\n",
      "backcall                  0.2.0              pyh9f0ad1d_0    conda-forge\n",
      "bcrypt                    3.1.7                    pypi_0    pypi\n",
      "bleach                    3.1.5              pyh9f0ad1d_0    conda-forge\n",
      "boto3                     1.14.27                  pypi_0    pypi\n",
      "botocore                  1.17.27                  pypi_0    pypi\n",
      "brotlipy                  0.7.0           py36h7b6447c_1000  \n",
      "ca-certificates           2020.6.20            hecda079_0    conda-forge\n",
      "cached-property           1.5.1                    pypi_0    pypi\n",
      "cairo                     1.14.12              h7636065_2  \n",
      "certifi                   2020.6.20        py36h9f0ad1d_0    conda-forge\n",
      "cffi                      1.14.0           py36he30daa8_1  \n",
      "chardet                   3.0.4           py36h9f0ad1d_1006    conda-forge\n",
      "colorama                  0.4.3                    pypi_0    pypi\n",
      "cryptography              2.5              py36hb7f436b_1    conda-forge\n",
      "dbus                      1.13.6               he372182_0    conda-forge\n",
      "decorator                 4.4.2                      py_0    conda-forge\n",
      "defusedxml                0.6.0                      py_0    conda-forge\n",
      "distro                    1.5.0                    pypi_0    pypi\n",
      "docker                    4.2.2                    pypi_0    pypi\n",
      "docker-compose            1.26.2                   pypi_0    pypi\n",
      "dockerpty                 0.4.1                    pypi_0    pypi\n",
      "docopt                    0.6.2                    pypi_0    pypi\n",
      "docutils                  0.15.2                   py36_0  \n",
      "entrypoints               0.3             py36h9f0ad1d_1001    conda-forge\n",
      "environment-kernels       1.1.1                    pypi_0    pypi\n",
      "expat                     2.2.9                he6710b0_2  \n",
      "fontconfig                2.12.6               h49f89f6_0  \n",
      "freetype                  2.8                  hab7d2ae_1  \n",
      "fribidi                   1.0.9                h7b6447c_0  \n",
      "fsspec                    0.7.4                      py_0  \n",
      "gitdb                     4.0.5                    pypi_0    pypi\n",
      "gitpython                 3.1.7                    pypi_0    pypi\n",
      "glib                      2.65.0               h3eb4bd4_0  \n",
      "graphite2                 1.3.14               h23475e2_0  \n",
      "graphviz                  2.40.1               h25d223c_0  \n",
      "gst-plugins-base          1.12.5               hde13a9d_0    conda-forge\n",
      "gstreamer                 1.12.4               hb53b477_0  \n",
      "harfbuzz                  1.7.6                h5f0a787_1  \n",
      "hdijupyterutils           0.12.9                     py_0    conda-forge\n",
      "icu                       58.2                 he6710b0_3  \n",
      "idna                      2.10                       py_0  \n",
      "importlib-metadata        1.7.0            py36h9f0ad1d_0    conda-forge\n",
      "importlib_metadata        1.7.0                         0    conda-forge\n",
      "ipykernel                 5.3.4            py36h95af2a2_0    conda-forge\n",
      "ipython                   7.16.1           py36h95af2a2_0    conda-forge\n",
      "ipython_genutils          0.2.0                      py_1    conda-forge\n",
      "ipywidgets                7.5.1                      py_0    conda-forge\n",
      "jedi                      0.17.2           py36h9f0ad1d_0    conda-forge\n",
      "jinja2                    2.11.2             pyh9f0ad1d_0    conda-forge\n",
      "jmespath                  0.10.0                     py_0  \n",
      "jpeg                      9b                   h024ee3a_2  \n",
      "json5                     0.9.5                    pypi_0    pypi\n",
      "jsonschema                3.2.0            py36h9f0ad1d_1    conda-forge\n",
      "jupyter                   1.0.0                      py_2    conda-forge\n",
      "jupyter_client            6.1.6                      py_0    conda-forge\n",
      "jupyter_console           6.1.0                      py_1    conda-forge\n",
      "jupyter_core              4.6.3            py36h9f0ad1d_1    conda-forge\n",
      "jupyterlab                1.2.16                   pypi_0    pypi\n",
      "jupyterlab-git            0.10.1                   pypi_0    pypi\n",
      "jupyterlab-server         1.2.0                    pypi_0    pypi\n",
      "krb5                      1.14.6                        0    conda-forge\n",
      "ld_impl_linux-64          2.33.1               h53a641e_7  \n",
      "libblas                   3.8.0               17_openblas    conda-forge\n",
      "libcblas                  3.8.0               17_openblas    conda-forge\n",
      "libffi                    3.3                  he6710b0_2  \n",
      "libgcc-ng                 9.1.0                hdf63c60_0  \n",
      "libgfortran-ng            7.5.0                hdf63c60_6    conda-forge\n",
      "liblapack                 3.8.0               17_openblas    conda-forge\n",
      "libopenblas               0.3.10          pthreads_hb3c22a3_4    conda-forge\n",
      "libpng                    1.6.37               hbc83047_0  \n",
      "libpq                     9.6.3                         0    conda-forge\n",
      "libprotobuf               3.12.3               h8b12597_2    conda-forge\n",
      "libsodium                 1.0.17               h516909a_0    conda-forge\n",
      "libstdcxx-ng              9.1.0                hdf63c60_0  \n",
      "libtiff                   4.1.0                h2733197_1  \n",
      "libtool                   2.4.6                h7b6447c_5  \n",
      "libxcb                    1.14                 h7b6447c_0  \n",
      "libxml2                   2.9.10               he19cac6_1  \n",
      "lz4-c                     1.9.2                he6710b0_0  \n",
      "markupsafe                1.1.1            py36h8c4c3a4_1    conda-forge\n",
      "mistune                   0.8.4           py36h8c4c3a4_1001    conda-forge\n",
      "mock                      4.0.2            py36h9f0ad1d_0    conda-forge\n",
      "nb_conda                  2.2.1                    py36_2    conda-forge\n",
      "nb_conda_kernels          2.2.3                    py36_0    conda-forge\n",
      "nbconvert                 5.6.1            py36h9f0ad1d_1    conda-forge\n",
      "nbdime                    1.1.0                    pypi_0    pypi\n",
      "nbexamples                0.0.0                    pypi_0    pypi\n",
      "nbformat                  5.0.7                      py_0    conda-forge\n",
      "nbserverproxy             0.3.2                    pypi_0    pypi\n",
      "ncurses                   5.9                          10    conda-forge\n",
      "nodejs                    10.13.0              he6710b0_0  \n",
      "nose                      1.3.7           py36h9f0ad1d_1004    conda-forge\n",
      "notebook                  5.7.10           py36h9f0ad1d_0    conda-forge\n",
      "numpy                     1.19.1           py36h7314795_0    conda-forge\n",
      "openjdk                   8.0.152              h7b6447c_3  \n",
      "openssl                   1.0.2u               h516909a_0    conda-forge\n",
      "packaging                 20.4               pyh9f0ad1d_0    conda-forge\n",
      "pandas                    0.22.0                   pypi_0    pypi\n",
      "pandoc                    2.10.1               h516909a_0    conda-forge\n",
      "pandocfilters             1.4.2                      py_1    conda-forge\n",
      "pango                     1.42.0               h377f3fa_0  \n",
      "paramiko                  2.7.1                    pypi_0    pypi\n",
      "parso                     0.7.1              pyh9f0ad1d_0    conda-forge\n",
      "pcre                      8.44                 he6710b0_0  \n",
      "pexpect                   4.8.0            py36h9f0ad1d_1    conda-forge\n",
      "pickleshare               0.7.5           py36h9f0ad1d_1001    conda-forge\n",
      "pid                       3.0.4                    pypi_0    pypi\n",
      "pip                       20.1.1                   pypi_0    pypi\n",
      "pixman                    0.40.0               h7b6447c_0  \n",
      "plotly                    4.9.0              pyh9f0ad1d_0    conda-forge\n",
      "prometheus_client         0.8.0              pyh9f0ad1d_0    conda-forge\n",
      "prompt-toolkit            3.0.5                      py_1    conda-forge\n",
      "prompt_toolkit            3.0.5                         1    conda-forge\n",
      "protobuf                  3.12.3           py36h831f99a_0    conda-forge\n",
      "protobuf3-to-dict         0.1.5                    pypi_0    pypi\n",
      "psutil                    5.7.2                    pypi_0    pypi\n",
      "psycopg2                  2.7.5            py36hdffb7b8_2    conda-forge\n",
      "ptyprocess                0.6.0                   py_1001    conda-forge\n",
      "py4j                      0.10.7                   pypi_0    pypi\n",
      "pyasn1                    0.4.8                    pypi_0    pypi\n",
      "pycparser                 2.20                       py_2  \n",
      "pygal                     2.4.0                    pypi_0    pypi\n",
      "pygments                  2.6.1                      py_0    conda-forge\n",
      "pykerberos                1.2.1            py36h14c3975_0  \n",
      "pynacl                    1.4.0                    pypi_0    pypi\n",
      "pyopenssl                 19.0.0                   py36_0    conda-forge\n",
      "pyparsing                 2.4.7              pyh9f0ad1d_0    conda-forge\n",
      "pyqt                      5.6.0           py36h13b7fb3_1008    conda-forge\n",
      "pyrsistent                0.16.0           py36h8c4c3a4_0    conda-forge\n",
      "pysocks                   1.7.1                    py36_0  \n",
      "pyspark                   2.3.4                    pypi_0    pypi\n",
      "python                    3.6.5                         1    conda-forge\n",
      "python-dateutil           2.8.1                      py_0  \n",
      "python-dotenv             0.14.0                   pypi_0    pypi\n",
      "python_abi                3.6                     1_cp36m    conda-forge\n",
      "pytz                      2020.1             pyh9f0ad1d_0    conda-forge\n",
      "pyyaml                    5.3.1                    pypi_0    pypi\n",
      "pyzmq                     19.0.1           py36h9947dbf_0    conda-forge\n",
      "qt                        5.6.2               h974d657_12  \n",
      "qtconsole                 4.7.5              pyh9f0ad1d_0    conda-forge\n",
      "qtpy                      1.9.0                      py_0    conda-forge\n",
      "readline                  7.0                           0    conda-forge\n",
      "requests                  2.24.0             pyh9f0ad1d_0    conda-forge\n",
      "requests-kerberos         0.12.0           py36h9f0ad1d_1    conda-forge\n",
      "retrying                  1.3.3                      py_2    conda-forge\n",
      "rsa                       4.5                      pypi_0    pypi\n",
      "s3fs                      0.4.2                      py_0  \n",
      "s3transfer                0.3.3                    py36_0  \n",
      "sagemaker                 1.71.0                   pypi_0    pypi\n",
      "sagemaker-experiments     0.1.20                   pypi_0    pypi\n",
      "sagemaker-nbi-agent       1.0                      pypi_0    pypi\n",
      "sagemaker-pyspark         1.3.2.post0              pypi_0    pypi\n",
      "scipy                     1.5.2                    pypi_0    pypi\n",
      "send2trash                1.5.0                      py_0    conda-forge\n",
      "setuptools                49.2.0                   py36_0  \n",
      "sip                       4.18.1          py36hf484d3e_1000    conda-forge\n",
      "six                       1.15.0                     py_0  \n",
      "smdebug-rulesconfig       0.1.4                    pypi_0    pypi\n",
      "smmap                     3.0.4                    pypi_0    pypi\n",
      "sparkmagic                0.15.0                     py_0    conda-forge\n",
      "sqlite                    3.20.1                        2    conda-forge\n",
      "terminado                 0.8.3            py36h9f0ad1d_1    conda-forge\n",
      "testpath                  0.4.4                      py_0    conda-forge\n",
      "texttable                 1.6.2                    pypi_0    pypi\n",
      "tk                        8.6.10               hbc83047_0  \n",
      "tornado                   6.0.4            py36h8c4c3a4_1    conda-forge\n",
      "traitlets                 4.3.3            py36h9f0ad1d_1    conda-forge\n",
      "urllib3                   1.22                     pypi_0    pypi\n",
      "wcwidth                   0.2.5              pyh9f0ad1d_0    conda-forge\n",
      "webencodings              0.5.1                      py_1    conda-forge\n",
      "websocket-client          0.57.0                   pypi_0    pypi\n",
      "wheel                     0.34.2                   py36_0  \n",
      "widgetsnbextension        3.5.1            py36h9f0ad1d_1    conda-forge\n",
      "xz                        5.2.5                h7b6447c_0  \n",
      "zeromq                    4.3.2                he1b5a44_2    conda-forge\n",
      "zipp                      3.1.0                      py_0    conda-forge\n",
      "zlib                      1.2.11               h7b6447c_3  \n",
      "zstd                      1.4.5                h0b5b093_0  \n"
     ]
    }
   ],
   "source": [
    "! conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import html.parser\n",
    "import gensim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import faiss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, '..')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "from utils.text_processing import apply_regexes, unescape_html, remove_html_tags, remove_line_breaks, preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['Text'] + ' ' + df['Summary']\n",
    "df['cleaned_text'].fillna(\"\", inplace=True)\n",
    "\n",
    "df['cleaned_text'] = df['cleaned_text'] \\\n",
    "    .apply(lambda x: unescape_html(remove_html_tags(remove_line_breaks(x), 'html.parser')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_text'] = df['cleaned_text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word To Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "review_text =  [word_tokenize(review) for review in df['preprocessed_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "review2vec = gensim.models.Word2Vec(\n",
    "    review_text, min_count=10, size=100, workers=10, sg=1, iter=10, window=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length for review2vec: 26959\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocab length for review2vec: {len(review2vec.wv.vocab)}')\n",
    "review2vec.save(\"../model/review2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('expiration', 0.8689521551132202),\n",
       " ('expires', 0.8167486786842346),\n",
       " ('date', 0.8046296834945679),\n",
       " ('expired', 0.7890604734420776),\n",
       " ('expiring', 0.7793307900428772),\n",
       " ('exp', 0.7547811269760132),\n",
       " ('oct', 0.7090200185775757),\n",
       " ('august', 0.702245831489563),\n",
       " ('stale', 0.6954241991043091),\n",
       " ('expiry', 0.6935100555419922)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review2vec.wv.most_similar('expire')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Most Similar Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_vector(word2vec_model, words):\n",
    "\n",
    "    if type(words) != str:\n",
    "        return np.zeros(word2vec_model.wv.vector_size, )\n",
    "    else:\n",
    "        words = words.split()\n",
    "        words = [word for word in words if word in word2vec_model.wv.vocab]\n",
    "        if len(words) >= 1:\n",
    "            return np.mean(word2vec_model.wv[words], axis=0)\n",
    "        else:\n",
    "            return np.zeros(word2vec_model.wv.vector_size, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embed = resample(df['preprocessed_text'], replace=False, n_samples=100000)\n",
    "review_text = df_embed.tolist()\n",
    "\n",
    "review_vector = np.asarray(\n",
    "    df_embed.apply(lambda x: get_mean_vector(review2vec, x)).tolist()\n",
    ")\n",
    "\n",
    "# normalize vectors\n",
    "review_vector_normed = np.copy(review_vector)\n",
    "nonzero_idx = np.sum(review_vector, axis=1) != 0\n",
    "review_vector_normed[nonzero_idx] = review_vector[nonzero_idx] / np.linalg.norm(\n",
    "    review_vector[nonzero_idx], axis=-1, keepdims=True\n",
    ")\n",
    "\n",
    "review_vector_normed = review_vector_normed.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Finding nearest neighbor\n",
    "\"\"\"\n",
    "faiss_index = faiss.IndexFlatIP(review_vector_normed.shape[1])\n",
    "faiss_index.add(review_vector_normed)\n",
    "top_k = 5\n",
    "S, I = faiss_index.search(review_vector_normed, top_k)\n",
    "get_review = np.vectorize(lambda x: review_text[x])\n",
    "nn_text = get_review(I)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame({'preprocessed_text': df_embed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval[\"nn_score\"] = [score for score in S]\n",
    "df_eval[\"nn_text\"] = [text for text in nn_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.to_csv(\n",
    "    \"../model/review2vec_eval.csv\", sep='\\t', index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Move this to utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Score'] <= 3, 'ReviewSentiment'] = 0\n",
    "df.loc[df['Score'] > 3, 'ReviewSentiment'] = 1\n",
    "\n",
    "df['ReviewSentiment'] = df['ReviewSentiment'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative samples: 124677\n",
      "Number of positive samples: 443777\n"
     ]
    }
   ],
   "source": [
    "negative = df[df['ReviewSentiment']==0]\n",
    "positive = df[df['ReviewSentiment']==1]\n",
    "print('Number of negative samples:', len(negative))\n",
    "print('Number of positive samples:', len(positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive downsampled: 124677\n",
      "Total Number of rows after downsampling: 249354\n"
     ]
    }
   ],
   "source": [
    "positive_downsampled = resample(positive, replace=True, # sample with replacement\n",
    "                                n_samples=len(negative), # match number in minority class\n",
    "                                random_state=1)\n",
    "print('Number of positive downsampled:', len(positive_downsampled))\n",
    "\n",
    "downsampled = pd.concat([negative, positive_downsampled])\n",
    "print('Total Number of rows after downsampling:', len(downsampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 199483\n",
      "Number of test samples: 49871\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    downsampled['preprocessed_text'], \n",
    "    downsampled['ReviewSentiment'], \n",
    "    test_size=0.2, \n",
    "    random_state=1, \n",
    "    stratify=downsampled['ReviewSentiment']\n",
    ")\n",
    "\n",
    "print('Number of train samples:', len(x_train))\n",
    "print('Number of test samples:', len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Review2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    review2vec\n",
    "except NameError:\n",
    "    review2vec = gensim.models.Word2Vec.load(\"../model/review2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the reviews words\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(downsampled['preprocessed_text'])\n",
    "\n",
    "# creating embedding matrix for directly feeding to Embedding() layer of Keras\n",
    "\n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, 100))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    \n",
    "    if i > num_words:\n",
    "        continue\n",
    "    \n",
    "    if word in review2vec.wv.vocab:\n",
    "        embedding_vector = review2vec.wv[word]\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout, Conv1D, MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 60, 100)           7115400   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 60, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 7,196,002\n",
      "Trainable params: 80,602\n",
      "Non-trainable params: 7,115,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_review2vec_keras = Sequential()\n",
    "\n",
    "adam = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999)\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    len(tokenizer.word_index)+1,\n",
    "    100,\n",
    "    embeddings_initializer=Constant(embedding_matrix),\n",
    "    input_length=60,\n",
    "    trainable=False\n",
    ")\n",
    "\n",
    "model_review2vec_keras.add(embedding_layer)\n",
    "model_review2vec_keras.add(Dropout(0.2))\n",
    "\n",
    "model_review2vec_keras.add(LSTM(100))\n",
    "model_review2vec_keras.add(Dropout(0.2))\n",
    "\n",
    "model_review2vec_keras.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "optimizer = Adam(lr=1e-3, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model_review2vec_keras.compile(loss=loss, optimizer=adam, metrics=['accuracy'])\n",
    "model_review2vec_keras.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "x_train_tokenized = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tokenized = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 199483 arrays: [array([[  30],\n       [  30],\n       [ 750],\n       [ 148],\n       [   3],\n       [ 125],\n       [ 836],\n       [ 282],\n       [ 804],\n       [  11],\n       [   8],\n       [ 255],\n       [1771],\n    ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-a0c58a54d344>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     epochs=10)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# TODO: will fix later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 199483 arrays: [array([[  30],\n       [  30],\n       [ 750],\n       [ 148],\n       [   3],\n       [ 125],\n       [ 836],\n       [ 282],\n       [ 804],\n       [  11],\n       [   8],\n       [ 255],\n       [1771],\n    ..."
     ]
    }
   ],
   "source": [
    "model_review2vec_keras_trained = model_review2vec_keras.fit(\n",
    "    x_train_tokenized, \n",
    "    y_train, \n",
    "    batch_size=16, #512\n",
    "    epochs=10)\n",
    "\n",
    "# TODO: will fix later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained_word2vec_history.evaluate(x_test_tokenized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline With LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train count vector: (199483, 65118)\n",
      "Shape of test count vector: (49871, 65118)\n"
     ]
    }
   ],
   "source": [
    "tfidfv = TfidfVectorizer(max_df=100000)\n",
    "fv_train = tfidfv.fit_transform(x_train)\n",
    "fv_test = tfidfv.transform(x_test)\n",
    "\n",
    "print('Shape of train count vector:', fv_train.shape)\n",
    "print('Shape of test count vector:', fv_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.908\n",
      "Test Accuracy: 0.889\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2', random_state=1, solver='sag', max_iter=1000, C=2)\n",
    "lr.fit(fv_train, y_train)\n",
    "\n",
    "train_acc = lr.score(fv_train, y_train)\n",
    "print('Train Accuracy: %.3f' %(train_acc))\n",
    "\n",
    "test_acc = lr.score(fv_test, y_test)\n",
    "print('Test Accuracy: %0.3f' % (test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Review2Vec With Mean Vectors (By Review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = review2vec.wv\n",
    "w2v_model = review2vec\n",
    "w2v_dim = w2v_model.wv.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from gensim.models.keyedvectors import BaseKeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gensim_word_index(kv:BaseKeyedVectors, oov_token:Optional[str]=None, num_words:Optional[int]=None):\n",
    "    \"\"\" Get word index from gensim keyed vectors so word indices are sorted by frequency.\n",
    "    Modified from https://github.com/keras-team/keras-preprocessing/blob/d7225f269ec28737d1fda036c2d26277b5863f87/keras_preprocessing/text.py#L199\n",
    "    :return: Two dictionaries of word to index and index to word.\n",
    "    :param kv: gensim Keyed Vectors\n",
    "    :param oov_token: Optional. A Special string to represent out-of-vocabulary token, e.g. <unk>.\n",
    "    :param num_words: Optioanl. Number of most frequent tokens to keep. If None, keep all.\n",
    "    \"\"\"\n",
    "    word_counts = {}\n",
    "    vocab = kv.vocab\n",
    "    for w in vocab:\n",
    "        word_counts[w] = vocab[w].count\n",
    "    wcounts = list(word_counts.items())\n",
    "    wcounts.sort(key=lambda x: x[1], reverse=True)\n",
    "    # first token is the out-of-vocabulary token if specified\n",
    "    if oov_token is None:\n",
    "        sorted_voc = []\n",
    "    else:\n",
    "        sorted_voc = [oov_token]\n",
    "    # keep top frequent tokens\n",
    "    if num_words is not None:\n",
    "        wcounts = wcounts[:num_words]\n",
    "    sorted_voc.extend(wc[0] for wc in wcounts)\n",
    "\n",
    "    # note that index 0 is reserved, never assigned to an existing word\n",
    "    # Fixme: index 0 can be assigned to oov token\n",
    "    word_index = dict(\n",
    "        zip(sorted_voc, list(range(1, len(sorted_voc) + 1))))\n",
    "\n",
    "    index_word = {c: w for w, c in word_index.items()}\n",
    "\n",
    "    return word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainedTextTokenizer(Tokenizer):\n",
    "    \"\"\"Use a pretrained gensim keyed vector as text tokenizer, which transforms list of strings to list of indices\"\"\"\n",
    "    \n",
    "    def fit_on_pretrained_embedding(self, gensim_kv: dict):\n",
    "        \"\"\"Wrapper for :func:`get_gensim_word_index`. Providing 2 attributes needed for the Base class\n",
    "        :param gensim_kv:\n",
    "        \"\"\"\n",
    "        self.word_index, self.index_word = get_gensim_word_index(\n",
    "            gensim_kv,\n",
    "            oov_token=self.oov_token,\n",
    "            num_words=self.num_words\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenizer_fit(df, gensim_kv: dict=None, num_words=None):\n",
    "    \"\"\"\n",
    "    :param df: input DataFrame\n",
    "    :param gensim_kv: pretrained gensim keyedvectors\n",
    "    :return: dict of text tokenizers\n",
    "    \"\"\"\n",
    "    text_tokenizer = dict()\n",
    "    \n",
    "    filters = '!\"#$%&()*_+,-./:;<=>?@[\\]^`{|}~'\n",
    "    lower = True\n",
    "\n",
    "    tokenizer = PreTrainedTextTokenizer(filters=filters, lower=lower, num_words=num_words)\n",
    "    tokenizer.fit_on_pretrained_embedding(gensim_kv=gensim_kv)\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenizer_transform(df, tokenizer, max_len=6):\n",
    "    \"\"\"\n",
    "    Transform all features\n",
    "    :return: tuple of (cat features, cat embed features, text features)\n",
    "    \"\"\"\n",
    "    text_padded = dict()\n",
    "    \n",
    "    text_sequences = tokenizer.texts_to_sequences(df['preprocessed_text'])\n",
    "    # if not passed any value, then set default maxlen=6\n",
    "\n",
    "    text_padded = pad_sequences(text_sequences, maxlen=max_len,\n",
    "                                padding='post', truncating='post')\n",
    "    return text_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = ?\n",
    "\n",
    "text_tokenizer = text_tokenizer_fit(df, gensim_kv=vocab)\n",
    "\n",
    "# featurization\n",
    "text_padded = text_tokenizer_transform(df_train, text_tokenizer)\n",
    "text_dims = text_padded.shape[1]\n",
    "\n",
    "# use text_padded as `x_train`\n",
    "x_train = ?\n",
    "y_train = ?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = kwargs.pop(\"loss\", \"default\")\n",
    "\n",
    "input_layers, output_layers = concatenate_features(\n",
    "    text_cols, text_tokenizer, text_dims, w2v_model, w2v_dim,\n",
    ")\n",
    "\n",
    "\n",
    "model = build_mlp_model(self.model_type, cat_features_dim, self.cat_embed_cols,\n",
    "                                 self.text_cols, self.encoders['text'], text_dims,\n",
    "                                 self.w2v_model, self.w2v_dim,\n",
    "                                 num_classes,\n",
    "                                 cat_embed_dims=cat_embed_dims,\n",
    "                                 cat_embed_embed_dims=cat_embed_embed_dims,\n",
    "                                 units_list = units_list,\n",
    "                                 loss=loss,\n",
    "                                 user2vec_model=self.user2vec_model,\n",
    "                                 user2vec_model_dim=self.user2vec_model_dim,\n",
    "                                 userid_mapping=userid_mapping,\n",
    "                                 learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_embed_layer(text_col, tokenizer, text_dim, embed_dim,\n",
    "                           text_w2v=None, pre_trained=False):\n",
    "    \"\"\"\n",
    "    :param text_col: text column name to be embeded (string)\n",
    "    :param tokenizer: keras tokenizer for the column\n",
    "    :param text_dim: length of the padded text input (e.g. maxlen=6, then this value is 6)\n",
    "    :param embed_dim: embedding dimension\n",
    "    :param text_w2v: pre-trained word2vec model\n",
    "    :param pre_trained: train the embedding E2E if False; else use the pre-trained w2v model\n",
    "    :return: input & output layer\n",
    "    \"\"\"\n",
    "    num_word = len(tokenizer.word_index)\n",
    "    embedding_matrix = np.zeros((num_word+1, embed_dim))\n",
    "    embed_input = Input(shape=(text_dim,), name=text_col+'_input')\n",
    "    if pre_trained:\n",
    "        kv = text_w2v.wv\n",
    "        for word, i in tokenizer.word_index.items():\n",
    "            if word in kv.vocab:\n",
    "                embedding_matrix[i] = kv[word]\n",
    "        # Todo: remove these. Not valid anymore\n",
    "        # n_no_match = np.sum(np.sum(embedding_matrix, axis=1) == 0)\n",
    "        # print(f'Null word embeddings: {n_no_match}, {round(n_no_match/num_word*100,2)}% of total unique words' )\n",
    "        embed_output = Embedding(embedding_matrix.shape[0],  # or len(word_index) + 1\n",
    "                              embedding_matrix.shape[1],  # or EMBEDDING_DIM,\n",
    "                              weights=[embedding_matrix],\n",
    "                              input_length=text_dim,\n",
    "                              trainable=False,\n",
    "                              name=text_col+'_embedding')(embed_input)\n",
    "    else:\n",
    "        embed_output = Embedding(embedding_matrix.shape[0],\n",
    "                              embedding_matrix.shape[1],\n",
    "                              input_length=text_dim,\n",
    "                              trainable=True,\n",
    "                              name=text_col+'_embedding')(embed_input)\n",
    "    embed_output = Flatten()(embed_output)\n",
    "    embed_output = Dropout(0.2)(embed_output)\n",
    "    return embed_input, embed_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 199483\n",
      "Number of test samples: 49871\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    downsampled['preprocessed_text'], \n",
    "    downsampled['ReviewSentiment'], \n",
    "    test_size=0.2, \n",
    "    random_state=1, \n",
    "    stratify=downsampled['ReviewSentiment']\n",
    ")\n",
    "\n",
    "print('Number of train samples:', len(x_train))\n",
    "print('Number of test samples:', len(x_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train count vector: (199483, 65118)\n",
      "Shape of test count vector: (49871, 65118)\n"
     ]
    }
   ],
   "source": [
    "tfidfv = TfidfVectorizer(max_df=100000)\n",
    "fv_train = tfidfv.fit_transform(x_train)\n",
    "fv_test = tfidfv.transform(x_test)\n",
    "\n",
    "print('Shape of train count vector:', fv_train.shape)\n",
    "print('Shape of test count vector:', fv_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.908\n",
      "Test Accuracy: 0.889\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2', random_state=1, solver='sag', max_iter=1000, C=2)\n",
    "lr.fit(fv_train, y_train)\n",
    "\n",
    "train_acc = lr.score(fv_train, y_train)\n",
    "print('Train Accuracy: %.3f' %(train_acc))\n",
    "\n",
    "test_acc = lr.score(fv_test, y_test)\n",
    "print('Test Accuracy: %0.3f' % (test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
